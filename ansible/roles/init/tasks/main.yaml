---

- name: Check that the python link exists
  stat:
      path: '/usr/bin/python'
  register: python_stat_result

- name: create soft link to /usr/bin/python
  file:
    src: '/usr/bin/python3'
    dest: '/usr/bin/python'
    state: link

- name: create group application
  group:
    name: '{{ application_group }}'
    state: present

- name: create tmpfs mount directory
  file:
     path: '{{ tmpfs_dir }}'
     state: directory 

- name: create  fstab entry 
  lineinfile: 
     dest: /etc/fstab
     line: tmpfs  /mnt/tmpfs  tmpfs   size={{ tmpfs_size }},mode=0777  0  0

- name: create  /etc/hosts entry
  lineinfile:
     dest: /etc/hosts
     line: '{{ item.ip }} {{ item.host }}'
  loop:
     - { ip: '{{ virtual_ip }}', host: '{{ keyclock_server }}'  }
     - { ip: '{{ virtual_ip }}', host: '{{ postgres_server }}'  }
     - { ip: '{{ virtual_ip }}', host: '{{ hive_metastore_server }}'  }
     - { ip: '{{ virtual_ip }}', host: '{{ superset_server }}'  }
     - { ip: '{{ virtual_ip }}', host: '{{ airflow_server }}'  }
     - { ip: '{{ virtual_ip }}', host: '{{ jupyterhub_server }}'  }
          

- name: create tmpfs mount point  
  command: mount -a 

- name: create tmpfs mount directory
  file:
     path: '{{ tmpfs_dir }}'
     state: directory 
     mode: '0770'
     group: '{{ application_group }}'

- name: Install jre
  apt:
    name: '{{ java_jdk }}'
    update_cache: yes

- name: Create user hadoop
  user:
    name: '{{ hadoop_user }}'
    group: '{{ application_group }}'
    shell: /bin/bash

- name: Create user minio-user
  user:
    name: '{{ minio_user }}'
    group: '{{ application_group }}'
    shell: /bin/bash  

- name: Create user spark
  user:
    name: '{{ spark_user }}'
    group: '{{ application_group }}'
    shell: /bin/bash

- name: Create user jupyterhub_user
  user:
    name: '{{ jupyterhub_user }}'
    group: '{{ application_group }}'
    shell: /bin/bash

- name: Create user hive
  user:
    name: '{{ hive_user }}'
    group: '{{ application_group }}'
    shell: /bin/bash

- name: Create user trino
  user:
    name: '{{ trino_user }}'
    group: '{{ application_group }}'
    shell: /bin/bash

- name: Create user airflow
  user:
    name: '{{ airflow_user }}'
    group: '{{ application_group }}'
    shell: /bin/bash

- name: Create user {{ keycloak_user }}
  user:
    name: '{{ keycloak_user }}'
    group: '{{ application_group }}'
    shell: /bin/bash

- name: Create user superset
  user:
    name: '{{ superset_user }}'
    group: '{{ application_group }}'
    shell: /bin/bash    

- name: Create user keycloak
  user:
    name: '{{ keycloak_user }}'
    group: '{{ application_group }}'
    shell: /bin/bash    


- name: Create user node_exporter
  user:
    name: '{{ node_exporter_user }}'
    group: '{{ application_group }}'
    shell: /bin/bash    

- name: Create user prometheus
  user:
    name: '{{ prometheus_user }}'
    group: '{{ application_group }}'
    shell: /bin/bash    

- name: Create user {{ grafana_user }}
  user:
    name: '{{ grafana_user }}'
    group: '{{ application_group }}'
    shell: /bin/bash    

- name: Create user {{ loki_user }}
  user:
    name: '{{ loki_user }}'
    group: '{{ application_group }}'
    shell: /bin/bash    

- name: Create user promtail user
  user:
    name: '{{ promtail_user }}'
    group: '{{ application_group }}'
    shell: /bin/bash    

- name: Check that the hadoop home exists
  stat:
      path: '{{ hadoop_home }}'
  register: hadoop_stat_result  

- name : extract hadoop  zip
  unarchive:
     src: "http://{{ groups['repo'][0] }}/download/{{ hadoop_file_name }}.tar.gz"
     dest: /opt
     remote_src: true
     owner: '{{ hadoop_user }}'
     group: '{{ application_group }}'
  when: not hadoop_stat_result.stat.exists

- name: create soft link to /opt/hadoop
  file:
    src: '/opt/{{ hadoop_file_name }}'
    dest: '{{ hadoop_home }}'
    state: link
    owner: '{{ hadoop_user }}'
    group: '{{ application_group }}'

- name: create {{ cert_dir }} dir
  file:
     path: '{{ cert_dir }}'
     state: directory

- name: copy  cert files
  template: src='{{ item.src }}' dest='{{ item.dest }}'
  loop:
     - { src: '{{ server_cert_file_name }}', dest: '{{ cert_dir }}/{{ server_cert_file_name }}'  }
     - { src: '{{ server_key_file_name }}', dest: '{{ cert_dir }}/{{ server_key_file_name }}'  }
     - { src: '{{ server_cert_key_file_name }}', dest: '{{ cert_dir }}/{{ server_cert_key_file_name }}'  }

- name: copy  {{ ca_cert_file_name }} to /usr/local/share/ca-certificates/
  template: src='{{ item.src }}' dest='{{ item.dest }}'
  loop:
     - { src: '{{ ca_cert_file_name }}', dest: '/usr/local/share/ca-certificates/{{ ca_cert_file_name }}' }
  notify: run certificates



- name:  create  jceks password
  shell: 'export JAVA_HOME={{ java_home }}; /opt/hadoop/bin/hadoop credential create  {{ item }}' 
  loop:
       - 'javax.jdo.option.ConnectionPassword -value {{ db_password }} -provider {{ jceks_provider }}'
       - 'fs.s3a.access.key -value {{ fs_s3a_access_key }} -provider {{ jceks_provider }}'
       - 'fs.s3a.secret.key -value {{ fs_s3a_secret_key }} -provider {{ jceks_provider }}'
  ignore_errors: yes       


- name: adjust secret.jceks
  file:
     path: '{{ item }}'
     group: '{{ application_group }}'
     mode: '0640'
  loop:
     - /usr/lib/java/secret.jceks
     - /usr/lib/java/.secret.jceks.crc      
     
