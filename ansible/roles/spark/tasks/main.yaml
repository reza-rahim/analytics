---

- name: Install jre
  apt:
    name: default-jre
    update_cache: yes


- name: Create a spark download directory if it does not exist
  file:
    path: '{{ spark_dowanload }}'
    state: directory

- name: Check that the spark_install_dir exists
  stat:
      path: '{{ spark_install_dir }}'
  register: stat_result
 
- name: create group spark
  group:
    name: spark
    state: present
  when: not stat_result.stat.exists

- name: Create user spark
  user:
    name: spark
    group: spark
    shell: /bin/bash
  when: not stat_result.stat.exists  


#- name : extract spark zip
# debug:
#   msg: tar -xvf {{ spark_dowanload }}/{{ spark_file }} --directory /opt
#

- name : extract spark zip
  unarchive:
     src: '{{ spark_file_url }}'
     dest: /opt
     remote_src: true
     owner: spark
     group: spark
  when: not stat_result.stat.exists  

- name: create soft link to /opt/spark   
  file:
    src: '/opt/{{ spark_file }}'
    dest: '/opt/spark'
    state: link
  when: not stat_result.stat.exists  

- name: get extra jar files 
  get_url:
     url: '{{ item }}'
     dest: /opt/spark/jars/
     owner: spark
     group: spark
  loop:
     - '{{ hadoop_aws }}'
     - '{{ aws_java_sdk_bundle }}'
     - '{{ iceberg_spark_runtime }}'
  when: not stat_result.stat.exists  

- name: copy  spark-master.service
  template: src='{{ item.src }}' dest='{{ item.dest }}'
  loop:
     - { src: 'spark-master.service.j2', dest: '/etc/systemd/system/spark-master.service'  }
     - { src: 'spark-env.sh.j2', dest: '/opt/spark/conf/spark-env.sh'  }
  when: >
    inventory_hostname in groups['spark_master'] and
    groups['spark_master'].index(inventory_hostname) == 0
  notify: restart start_master

- name: copy 
  template: src='{{ item.src }}' dest='{{ item.dest }}'
  loop:
     - { src: 'spark-worker.service.j2', dest: '/etc/systemd/system/spark-worker.service'  }
     - { src: 'spark-env.sh.j2', dest: '/opt/spark/conf/spark-env.sh'  }
  when: >
    inventory_hostname in groups['spark_worker']
  notify: restart start_worker

